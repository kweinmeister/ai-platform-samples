{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorFlow on Google Cloud AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how [TensorFlow](https://tensorflow.org) is used in multiple services throughout the [Google Cloud AI Platform](https://cloud.google.com/ai-platform).\n",
    "\n",
    "The first section shows how you can programatically setup Deep Learning VM and Notebook instances configured with TensorFlow.\n",
    "\n",
    "After that, you'll perform the following steps:\n",
    "* Build a TensorFlow model for the [MNIST dataset](http://yann.lecun.com/exdb/mnist/)\n",
    "* Package the model code in a container\n",
    "* Train the model in the Cloud AI Platform Training service\n",
    "* Deploy the model to the Cloud AI Platform Prediction service\n",
    "* Make predictions with the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must change these parameters\n",
    "\n",
    "PROJECT = 'change-me-123456'\n",
    "BUCKET = 'gs://change-me'\n",
    "REGION = 'us-west1'\n",
    "ZONE = 'us-west1-b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other parameters\n",
    "\n",
    "DLVM_NAME = 'my-dlvm'\n",
    "NOTEBOOK_NAME = 'my-notebook'\n",
    "MODEL_NAME = 'mnist'\n",
    "DLVM_IMAGE_FAMILY = 'tf2-ent-latest-cpu'\n",
    "TF_IMAGE_PROJECT = 'deeplearning-platform-release'\n",
    "TF_IMAGE_FAMILY = 'tf2-cpu'\n",
    "\n",
    "IMAGE_REPO_NAME = 'tf_gcp_custom_container'\n",
    "IMAGE_TAG = 'mnist'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT}/{IMAGE_REPO_NAME}:{IMAGE_TAG}'\n",
    "JOB_DIR = f'{BUCKET}/{MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Administration tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with a Deep Learning VM Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Google Compute Engine (GCE) instance using the TensorFlow image\n",
    "\n",
    "!gcloud compute instances create $DLVM_NAME \\\n",
    "  --zone=$ZONE \\\n",
    "  --image-family=$DLVM_IMAGE_FAMILY \\\n",
    "  --image-project=$TF_IMAGE_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that instance was created\n",
    "\n",
    "!gcloud compute instances list | grep $DLVM_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete instance\n",
    "\n",
    "!gcloud compute instances delete $DLVM_NAME \\\n",
    "  --zone=$ZONE \\\n",
    "  --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Notebooks API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install package\n",
    "\n",
    "!pip install google-cloud-notebooks --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from google.cloud.notebooks_v1beta1.services.notebook_service import NotebookServiceClient\n",
    "from google.cloud.notebooks_v1beta1.types import ListInstancesRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client\n",
    "\n",
    "client = NotebookServiceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the list of instances\n",
    "\n",
    "parent = f'projects/{PROJECT}/locations/{ZONE}'\n",
    "response = client.list_instances(ListInstancesRequest({\"parent\": parent}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the name of the first instance\n",
    "\n",
    "response.instances[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same command via CLI\n",
    "\n",
    "!gcloud beta notebooks instances list --location $ZONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bundle with model and training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for training files\n",
    "\n",
    "TRAIN_DIR = 'train'\n",
    "\n",
    "if os.path.isdir(TRAIN_DIR) is False:\n",
    "    os.mkdir(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create task.py file with training code\n",
    "\n",
    "task_template = \"\"\"import argparse\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--job-dir',\n",
    "                        default='',\n",
    "                        help='URL to store the job output')\n",
    "    parser.add_argument('--batch-size',\n",
    "                        type=int,\n",
    "                        default=32,\n",
    "                        help='input batch size for training (default: 32)')\n",
    "    parser.add_argument('--epochs',\n",
    "                        type=int,\n",
    "                        default=10,\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    parser.add_argument('--version',\n",
    "                        type=str,\n",
    "                        default=datetime.datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "                        help='Subdirectory where the model files will be saved')\n",
    "                        \n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Parse arguments\n",
    "    args = get_args()\n",
    "    batch_size = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    job_dir = args.job_dir\n",
    "    version = args.version\n",
    "    print('args: ', args)\n",
    "\n",
    "    # Load data\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    # Train model\n",
    "    model = get_model()\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "    model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "    # Export the model\n",
    "    export_path = os.path.join(job_dir, 'export', version)\n",
    "    model.save(export_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{TRAIN_DIR}/task.py', 'w') as f:\n",
    "    f.write(task_template.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a version identifier based on the current date and time\n",
    "\n",
    "VERSION = '{:%Y%m%d_%H%M%S}'.format(datetime.datetime.now())\n",
    "VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training code before deploying it\n",
    "\n",
    "!python $TRAIN_DIR/task.py --version $VERSION --epochs 1 --job-dir $JOB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile\n",
    "\n",
    "dockerfile_template = f\"\"\"FROM gcr.io/{TF_IMAGE_PROJECT}/{TF_IMAGE_FAMILY}\n",
    "WORKDIR /root\n",
    "COPY {TRAIN_DIR}/task.py /root/task.py\n",
    "ENTRYPOINT [\"python\", \"task.py\"]\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{TRAIN_DIR}/Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile_template.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build container image\n",
    "\n",
    "!docker build -f $TRAIN_DIR/Dockerfile -t $IMAGE_URI ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test container locally\n",
    "\n",
    "!docker run $IMAGE_URI --job-dir $JOB_DIR --epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push image to container registry if local test is successful\n",
    "\n",
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit AI Platform training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit training job\n",
    "\n",
    "JOB_NAME = 'custom_container_job_' + VERSION\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --region $REGION \\\n",
    "  --master-image-uri $IMAGE_URI \\\n",
    "  -- \\\n",
    "  --version=$VERSION \\\n",
    "  --job-dir=$JOB_DIR \\\n",
    "  --epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the job status, to ensure it has completed before continuing.\n",
    "\n",
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model to Prediction service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AI Platform Prediction model\n",
    "\n",
    "!gcloud ai-platform models create '{MODEL_NAME}' \\\n",
    "  --region='{REGION}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model version string with the current datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "MODEL_VERSION = 'v' + datetime.datetime.strftime(now, '%m%d%Y%H%M%S')\n",
    "MODEL_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify location of the model that was created by the training job\n",
    "\n",
    "MODEL_URI = os.path.join(JOB_DIR, 'export', VERSION)\n",
    "MODEL_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model version. This may take several minutes.\n",
    "\n",
    "!gcloud ai-platform versions create {MODEL_VERSION} \\\n",
    "  --model={MODEL_NAME} \\\n",
    "  --region={REGION} \\\n",
    "  --origin={MODEL_URI} \\\n",
    "  --staging-bucket={BUCKET} \\\n",
    "  --runtime-version=2.3 \\\n",
    "  --framework='TENSORFLOW' \\\n",
    "  --python-version=3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use service to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import googleapiclient.discovery\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from google.api_core.client_options import ClientOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to invoke the prediction service from\n",
    "# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/ml_engine/online_prediction/predict.py\n",
    "\n",
    "def predict_json(project, model, instances, version=None):\n",
    "    \"\"\"Send json data to a deployed model for prediction.\n",
    "    Args:\n",
    "        project (str): project where the AI Platform Model is deployed.\n",
    "        model (str): model name.\n",
    "        instances ([Mapping[str: Any]]): Keys should be the names of Tensors\n",
    "            your deployed model expects as inputs. Values should be datatypes\n",
    "            convertible to Tensors, or (potentially nested) lists of datatypes\n",
    "            convertible to tensors.\n",
    "        version: str, version of the model to target.\n",
    "    Returns:\n",
    "        Mapping[str: any]: dictionary of prediction results defined by the\n",
    "            model.\n",
    "    \"\"\"\n",
    "\n",
    "    name = 'projects/{}/models/{}'.format(project, model)\n",
    "\n",
    "    if version is not None:\n",
    "        name += '/versions/{}'.format(version)\n",
    "\n",
    "    response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': instances}\n",
    "    ).execute()\n",
    "\n",
    "    if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "\n",
    "    return response['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "\n",
    "endpoint = f'https://{REGION}-ml.googleapis.com'  # Use regional endpoint\n",
    "client_options = ClientOptions(api_endpoint=endpoint)\n",
    "service = googleapiclient.discovery.build('ml', 'v1', client_options=client_options, cache_discovery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a sample image from the test data\n",
    "\n",
    "sample = x_test[0]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(sample)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the prediction service with the image data\n",
    "\n",
    "response = predict_json(PROJECT, MODEL_NAME, sample.tolist())\n",
    "max_value = max(response[0])\n",
    "max_index = response[0].index(max_value)\n",
    "\n",
    "print(f'Predicted value: {max_index}')\n",
    "print(f'Confidence:      {round(max_value, 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model version resource\n",
    "!gcloud ai-platform versions delete {MODEL_VERSION} --model {MODEL_NAME} --region {REGION} --quiet \n",
    "\n",
    "# Delete model resource\n",
    "!gcloud ai-platform models delete {MODEL_NAME} --region {REGION} --quiet"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
